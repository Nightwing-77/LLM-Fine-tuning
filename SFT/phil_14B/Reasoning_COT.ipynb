{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12d64298-c273-4558-bb90-1026f79aab60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============================ ROCm System Management Interface ============================\n",
      "====================================== Product Info ======================================\n",
      "GPU[0]\t\t: Card Series: \t\tAMD Instinct MI300X VF\n",
      "GPU[0]\t\t: Card Model: \t\t0x74b5\n",
      "GPU[0]\t\t: Card Vendor: \t\tAdvanced Micro Devices, Inc. [AMD/ATI]\n",
      "GPU[0]\t\t: Card SKU: \t\tM3000100\n",
      "GPU[0]\t\t: Subsystem ID: \t0x74a1\n",
      "GPU[0]\t\t: Device Rev: \t\t0x00\n",
      "GPU[0]\t\t: Node ID: \t\t1\n",
      "GPU[0]\t\t: GUID: \t\t21947\n",
      "GPU[0]\t\t: GFX Version: \t\tgfx942\n",
      "==========================================================================================\n",
      "================================== End of ROCm SMI Log ===================================\n"
     ]
    }
   ],
   "source": [
    "!rocm-smi --showproductname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1d153ee-d9e6-4a60-9e6d-fff01ac4a96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 https://repo.radeon.com/amdgpu/30.10_rc1/ubuntu noble InRelease\n",
      "Hit:2 https://repo.radeon.com/graphics/7.0_rc1/ubuntu noble InRelease          \u001b[0m\n",
      "Hit:3 https://repo.radeon.com/rocm/apt/7.0_rc1 noble InRelease                 \u001b[0m\u001b[33m\n",
      "Hit:4 http://security.ubuntu.com/ubuntu noble-security InRelease               \u001b[0m\u001b[33m\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu noble InRelease                         \n",
      "Hit:6 http://archive.ubuntu.com/ubuntu noble-updates InRelease\n",
      "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu noble InRelease\n",
      "Hit:8 http://archive.ubuntu.com/ubuntu noble-backports InRelease\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "22 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "git is already the newest version (1:2.43.0-1ubuntu7.3).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 22 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!sudo apt update && sudo apt install git -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ddf4c0a-e473-49ba-b504-03759f8231dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U transformers\n",
    "%pip install -U datasets \n",
    "%pip install -U accelerate \n",
    "%pip install -U peft \n",
    "%pip install -U trl \n",
    "%pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bab181fa-3529-4297-874e-5a68aca5d4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.4.1+rocm6.0\n",
      "Uninstalling torch-2.4.1+rocm6.0:\n",
      "  Successfully uninstalled torch-2.4.1+rocm6.0\n",
      "Found existing installation: torchvision 0.19.1+rocm6.0\n",
      "Uninstalling torchvision-0.19.1+rocm6.0:\n",
      "  Successfully uninstalled torchvision-0.19.1+rocm6.0\n",
      "Found existing installation: torchaudio 2.4.1+rocm6.0\n",
      "Uninstalling torchaudio-2.4.1+rocm6.0:\n",
      "  Successfully uninstalled torchaudio-2.4.1+rocm6.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall torch torchvision torchaudio -y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e1383a9-cf4e-4a19-ba9a-ae09940d5d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/rocm6.0\n",
      "Collecting torch==2.4.1+rocm6.0\n",
      "  Downloading https://download.pytorch.org/whl/rocm6.0/torch-2.4.1%2Brocm6.0-cp312-cp312-linux_x86_64.whl (2363.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 GB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m  \u001b[33m0:00:37\u001b[0mm0:00:01\u001b[0m00:02\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.4.1+rocm6.0\n",
      "    Uninstalling torch-2.4.1+rocm6.0:\n",
      "      Successfully uninstalled torch-2.4.1+rocm6.0\n",
      "Successfully installed torch-2.4.1+rocm6.0\n",
      "Looking in indexes: https://download.pytorch.org/whl/rocm6.0\n",
      "Collecting torchvision==0.19.1+rocm6.0\n",
      "  Downloading https://download.pytorch.org/whl/rocm6.0/torchvision-0.19.1%2Brocm6.0-cp312-cp312-linux_x86_64.whl (65.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 MB\u001b[0m \u001b[31m123.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchvision\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.19.1+rocm6.0\n",
      "    Uninstalling torchvision-0.19.1+rocm6.0:\n",
      "      Successfully uninstalled torchvision-0.19.1+rocm6.0\n",
      "Successfully installed torchvision-0.19.1+rocm6.0\n",
      "Looking in indexes: https://download.pytorch.org/whl/rocm6.0\n",
      "Collecting torchaudio==2.4.1+rocm6.0\n",
      "  Downloading https://download.pytorch.org/whl/rocm6.0/torchaudio-2.4.1%2Brocm6.0-cp312-cp312-linux_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchaudio\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.4.1+rocm6.0\n",
      "    Uninstalling torchaudio-2.4.1+rocm6.0:\n",
      "      Successfully uninstalled torchaudio-2.4.1+rocm6.0\n",
      "Successfully installed torchaudio-2.4.1+rocm6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.4.1+rocm6.0 --index-url https://download.pytorch.org/whl/rocm6.0 --no-deps --force-reinstall --no-cache-dir\n",
    "!pip install torchvision==0.19.1+rocm6.0 --index-url https://download.pytorch.org/whl/rocm6.0 --no-deps --force-reinstall --no-cache-dir  \n",
    "!pip install torchaudio==2.4.1+rocm6.0 --index-url https://download.pytorch.org/whl/rocm6.0 --no-deps --force-reinstall --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13e84f51-dc6d-4239-8d7a-be2bb48431e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#downloaded the wrong torch lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3b992e2-6965-4865-ad90-f2890825b632",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets huggingface-hub scipy -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6589df54-9de3-4997-9078-52efedf77243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment check:\n",
      "ROCM_PATH: Not set\n",
      "HIP_VISIBLE_DEVICES: Not set\n",
      "\n",
      "PyTorch version: 2.4.1+rocm6.0\n",
      "ROCm available: True\n",
      "Device count: 1\n",
      "Current device: 0\n",
      "Device name: AMD Instinct MI300X VF\n",
      "Device properties: _CudaDeviceProperties(name='AMD Instinct MI300X VF', major=9, minor=4, gcnArchName='gfx942:sramecc+:xnack-', total_memory=196288MB, multi_processor_count=304)\n",
      "Test computation successful: torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "print(\"Environment check:\")\n",
    "print(f\"ROCM_PATH: {os.getenv('ROCM_PATH', 'Not set')}\")\n",
    "print(f\"HIP_VISIBLE_DEVICES: {os.getenv('HIP_VISIBLE_DEVICES', 'Not set')}\")\n",
    "\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"ROCm available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "    print(f\"Current device: {device}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(device)}\")\n",
    "    print(f\"Device properties: {torch.cuda.get_device_properties(device)}\")\n",
    "    \n",
    "    # Test tensor operations\n",
    "    x = torch.randn(3, 3).cuda()\n",
    "    y = torch.randn(3, 3).cuda()\n",
    "    z = x + y\n",
    "    print(f\"Test computation successful: {z.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a605ae84-fa2b-49d6-971e-0dce395ce345",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install trl -qq\n",
    "!pip install peft -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41d219c4-b875-40ff-b928-b1b3d6cf6580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be67d6de-313c-457b-81cb-f37fe54747a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e059cc0c-faa7-4ce7-bf3f-22a3262edf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from huggingface_hub import login\n",
    "login(token=os.environ[\"HUGGING_FACE_TOKEN\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25a94150-2f9a-4713-8cde-510efb8f8c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m⚠️  Warning: 'huggingface-cli whoami' is deprecated. Use 'hf auth whoami' instead.\u001b[0m\n",
      "Advik-7\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d37ca8b2-7b9e-4293-98b0-27c6a96f3dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf1a55b66c54c9a89add4863340a34d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROCm device: AMD Instinct MI300X VF\n",
      "Memory allocated: 27.383586883544922 GB\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# 1. Loading the Model and Tokenizer (AMD ROCm variant)\n",
    "model_dir = \"microsoft/Phi-4-reasoning-plus\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n",
    "\n",
    "# ROCm specific loading - explicit device placement instead of device_map=\"auto\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir, \n",
    "    torch_dtype=torch.bfloat16,  # ROCm supports bfloat16\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Move to ROCm device (ROCm uses cuda syntax)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Check ROCm memory usage\n",
    "print(\"ROCm device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "print(\"Memory allocated:\", torch.cuda.memory_allocated(0) / 1024**3, \"GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f2d2c2d-03da-4e1e-beba-ac20340a549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompt_style = \"\"\"\n",
    "<|im_start|>system<|im_sep|>\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request. \n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "<|im_end|>\n",
    "<|im_start|>user<|im_sep|>\n",
    "{}<|im_end|>\n",
    "<|im_start|>assistant<|im_sep|>\n",
    "<think>\n",
    "{}\n",
    "</think>\n",
    "{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9af29bfb-a05c-430d-9be7-109da220bb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"Open-ended Verifiable Question\"]\n",
    "    complex_cots = examples[\"Complex_CoT\"]\n",
    "    outputs = examples[\"Response\"]\n",
    "    texts = []\n",
    "    for input, cot, output in zip(inputs, complex_cots, outputs):\n",
    "        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60687b28-2ef9-4320-a9f6-e771a64e0ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"TheFinAI/Fino1_Reasoning_Path_FinQA\", split=\"train[0:1000]\"\n",
    ")\n",
    "dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    ")\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b87d73e3-7f31-4992-b004-50bd463597e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_prompt_style = \"\"\"\n",
    "<|im_start|>system<|im_sep|>\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request. \n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "<|im_end|>\n",
    "<|im_start|>user<|im_sep|>\n",
    "{}<|im_end|>\n",
    "<|im_start|>assistant<|im_sep|>\n",
    "<think>\n",
    "{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc6600f-2782-4c31-bf86-cb07c3ca9d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples\n",
    "print(\"Number of rows:\", len(dataset))\n",
    "\n",
    "# Inspect a random example\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2c7aa76-0de2-4a7f-b975-da284630be89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before fine-tuning:\n",
      "<think><|im_end|>We are asked: \"what was the percentage increase in the employee contribution from 2002 to 2003\". The context: \"retirement plan: The company has a 401(k) plan covering substantially all employees who meet certain age and employment requirements. Under the plan, the company matching contribution for periods prior to June 30, 2004 was 35% (35%) up to a maximum 5% (5%) of a participant's contributions. Effective July 1, 2004, the plan was amended to increase the company match to 50% (50%) up to a maximum 6% (6%) of a participant's contributions. The company contributed approximately $533,000, $825,000 and $979,000 to the plan for the years ended December 31, 2004, 2003 and 2002, respectively.\"\n",
      "\n",
      "Wait, the question: \"what was the percentage increase in the employee contribution from 2002 to 2003?\" But what exactly means employee contribution? It says \"employee contribution\" but in context, the provided numbers: For 2004, 2003, 2002, the company contributed amounts to the plan. But maybe the employee contributions are not provided? But we see \"the company contributed approx. $533,000, $825,000 and $979,000 to the plan for the years ended December 31, 2004, 2003 and 2002, respectively.\" But note: these numbers appear to be the company's contributions to the plan, not necessarily employee contributions. Wait re-read: \"retirement plan: The company has a 401(k) plan covering substantially all employees who meet certain age and employment requirements. Under the plan, the company matching contribution for periods prior to June 30, 2004 was 35% up to a maximum 5% of a participant's contributions. Effective July 1, 2004, the plan was amended to increase the company match to 50% up to a maximum 6% of a participant's contributions. The company contributed approximately $533,000, $825,000 and $979,000 to the plan for the years ended December 31, 2004, 2003 and 2002, respectively.\" But the question: \"what was the percentage increase in the employee contribution from 2002 to 2003?\" But there is no explicit employee contribution amounts provided. There are only amounts for company contributions. But perhaps the question is referencing the fact that the employee contribution is the maximum of 5% or 6% in each year? But the numbers provided are $533000, $825000 and $979000. Wait maybe the employee contributions are computed from the amounts contributed by the company? But the text says: \"The company contributed approximately $533,000, $825,000 and $979,000 to the plan for the years ended December 31, 2004, 2003 and 2002, respectively.\" And then the question: \"What was the percentage increase in the employee contribution from 2002 to 2003?\" But then maybe the question is computed by comparing the employee contribution amounts? But wait, are these employee contributions? The statement says \"the company contributed\", which is ambiguous. But then the plan details: \"Under the plan, the company matching contribution for periods prior to June 30, 2004 was 35% up to a maximum 5% of a participant's contributions.\" So if employee contributes, the company match is computed. But then the numbers given are the company contributions. They are not employee contributions. The question is likely: \"What was the percentage increase in the employee contribution from 2002 to 2003\" but we have numbers for the company contributions, not employees. Wait, the question seems to be asking: \"what was the percentage increase in the employee contribution from 2002 to 2003\" but then the text \"the company contributed approximately $533,000, $825,000 and $979,000 to the plan for the years ended December 31, 2004, 2003 and 2002, respectively\" might be used to calculate the employee contributions if we assume the employee contributions are the amounts on which the company match is computed. But then the employee's contribution would be computed as: company contribution / company match percentage (if we assume maximum contribution)? But the plan description is: \"Under the plan, the company matching contribution for periods prior to June 30, 2004 was 35% (35%) up to a maximum 5% of a participant's contributions.\" So if the company contributed $X, that is 35% of the employee's contribution? But that's not how matching works: The company matches a percentage of the employee's contribution, but the employee's contribution is presumably greater than the company contribution. Wait, re-read: \"Under the plan, the company matching contribution for periods prior to June 30, 2004 was 35% (35%) up to a maximum 5% of a participant's contributions.\" That is, if an employee contributes a certain amount, the company matches 35% of that contribution, but the match is limited to 5% of the employee's salary? Actually, it says \"up to a maximum 5% of a participant's contributions\", which might be interpreted as \"The company matches at a rate of 35% but only up to 5% of the participant's salary\"? Actually, no: \"35% of a participant's contributions\" but then \"up to a maximum 5% of a participant's contributions\" doesn't make sense because 35% is more than 5%. Let me re-read that text: \"Under the plan, the company matching contribution for periods prior to June 30, 2004 was\n"
     ]
    }
   ],
   "source": [
    "question = dataset[20]['Open-ended Verifiable Question']\n",
    "inputs = tokenizer(\n",
    "    [inference_prompt_style.format(question, \"\") + tokenizer.eos_token],\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)  # Use our ROCm device\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(\"Before fine-tuning:\")\n",
    "print(response[0].split(\"<|im_start|>assistant<|im_sep|>\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26f4dbe8-517e-4335-b518-4be21c983a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.12/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/opt/venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14e385b6-e4b0-45d6-8b0d-57c4b539f5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 11:59, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.445700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.362600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.281900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.321600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.342200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=1.350809295654297, metrics={'train_runtime': 720.7619, 'train_samples_per_second': 1.387, 'train_steps_per_second': 0.694, 'total_flos': 8.592683944836096e+16, 'train_loss': 1.350809295654297, 'epoch': 1.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"adamw_torch\",\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=0.2,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    group_by_length=True,\n",
    "    report_to=\"none\",\n",
    "    dataloader_pin_memory=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "935ec15d-b600-493f-ac7a-341d2aaf3353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine-tuning:\n",
      "<think><|im_end|>Alright, let's get started on figuring out the percentage increase in the employee contribution from 2002 to 2003. First off, we need to look at the contributions for each year. The data says that in 2002, the contributions were $979,000. Moving on to 2003, the contributions went up to $825,000.\n",
      "\n",
      "Now, I should calculate the difference between these two years. That's easy enough; I just subtract the 2002 amount from the 2003 amount. So, $825,000 minus $979,000 gives us -$154,000. Oh, it looks like there's a decrease here, not an increase.\n",
      "\n",
      "Hmm, let's pause here. The problem specifically asks about the percentage increase in the employee contribution. But according to the numbers we have, there's actually a decrease from 2002 to 2003. So, it's not about an increase at all.\n",
      "\n",
      "Let's double-check those numbers to make sure I didn't miss anything. Back in 2002, the contributions were indeed $979,000, and in 2003, it's $825,000. So, we're seeing a reduction, not an increase.\n",
      "\n",
      "Therefore, since the contributions have decreased, there's no percentage increase to calculate from 2002 to 2003. It's more like a decrease. Hmm, so I guess I'll just say there's no percentage increase, because the numbers clearly show a decrease.\n",
      "\n",
      "Alright, so to wrap this up, we found out that the employee contribution actually decreased from 2002 to 2003 by $154,000, so there's no percentage increase to speak of. That should be the correct conclusion.</think>Based on the data provided, employee contributions actually decreased from 2002 to 2003. In 2002, the contribution was $979,000, and in 2003, it dropped to $825,000. Since the numbers show a decrease rather than an increase, there is no percentage increase to calculate for this period.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = dataset[20]['Open-ended Verifiable Question']\n",
    "inputs = tokenizer(\n",
    "    [inference_prompt_style.format(question, \"\") + tokenizer.eos_token],\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(\"After fine-tuning:\")\n",
    "print(response[0].split(\"<|im_start|>assistant<|im_sep|>\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8dbb783e-e12c-4f3a-acf3-997d2c223547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f940eceba4e24b7dbcee8593d8c5fc80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90137cf17d54f648694ae0e0c9e5d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[2m2025-09-27T16:41:41.949092Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mStatus Code: 504. Retrying..., \u001b[1;33mrequest_id\u001b[0m\u001b[33m: \"\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /home/runner/work/xet-core/xet-core/cas_client/src/http_client.rs:227\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9417dab9f46646bc98524c6afe3eff4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as: Phi-4-Reasoning-Plus-FinQA-COT-ROCm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "new_model_name = \"Phi-4-Reasoning-Plus-FinQA-COT-ROCm\"\n",
    "model.push_to_hub(new_model_name)\n",
    "tokenizer.push_to_hub(new_model_name)\n",
    "print(f\"Model saved as: {new_model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b7b1cef-4369-4450-9e96-58aae384a364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ROCm memory cleanup\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32280061-1317-4148-aa44-e3dd65d31b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
